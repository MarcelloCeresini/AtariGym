{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "REAPLAY_MEMORY_SIZE = 10000 # should be 1000000\n",
    "REPLAY_START_SIZE = 500 # should be 50000\n",
    "experience_buffer = collections.deque([], maxlen=REAPLAY_MEMORY_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INIT_EPS = 1\n",
    "END_EPS = 0.1\n",
    "FINAL_EXPLORATION_FRAME = REAPLAY_MEMORY_SIZE\n",
    "\n",
    "DISCOUNT_FACTOR = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marcello/github/AtariGym/env/lib/python3.8/site-packages/flatbuffers/compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "2022-04-06 21:36:19.711469: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-06 21:36:19.738816: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-06 21:36:19.739159: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "A.L.E: Arcade Learning Environment (version 0.7.4+069f8bd)\n",
      "[Powered by Stella]\n",
      "/home/marcello/github/AtariGym/env/lib/python3.8/site-packages/gym/utils/seeding.py:138: DeprecationWarning: \u001b[33mWARN: Function `hash_seed(seed, max_bytes)` is marked as deprecated and will be removed in the future. \u001b[0m\n",
      "  deprecation(\n",
      "/home/marcello/github/AtariGym/env/lib/python3.8/site-packages/gym/utils/seeding.py:175: DeprecationWarning: \u001b[33mWARN: Function `_bigint_from_bytes(bytes)` is marked as deprecated and will be removed in the future. \u001b[0m\n",
      "  deprecation(\n",
      "/home/marcello/github/AtariGym/env/lib/python3.8/site-packages/keras/applications/efficientnet.py:288: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 12 input channels.\n",
      "  input_shape = imagenet_utils.obtain_input_shape(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-06 21:36:19.861869: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-06 21:36:19.862905: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-06 21:36:19.863171: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-06 21:36:19.863283: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-06 21:36:20.284148: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-06 21:36:20.284309: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-06 21:36:20.284424: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-06 21:36:20.284542: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3534 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4015264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marcello/github/AtariGym/env/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from random import sample\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "print(tf.config.list_physical_devices('GPU')[0])\n",
    "\n",
    "env = gym.make(\"ALE/Breakout-v5\")\n",
    "\n",
    "N_ACTIONS = env.action_space.n\n",
    "OBS_RANGE = int(env.observation_space.high_repr) - int(env.observation_space.low_repr)\n",
    "OBS_SHAPE = env.observation_space.shape\n",
    "# SKIP_FRAMES = [2, 3, 4] # possible addition --> skip randomly 2/3/4 frames instead of always 4\n",
    "N_INPUT_FRAMES = 4\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "input_shape = [*OBS_SHAPE]\n",
    "input_shape[-1] *= N_INPUT_FRAMES\n",
    "\n",
    "obs = np.zeros(input_shape)\n",
    "target = np.zeros(BATCH_SIZE)\n",
    "\n",
    "model = tf.keras.applications.efficientnet.EfficientNetB0(\n",
    "    include_top=True,\n",
    "    weights=None,\n",
    "    input_shape=input_shape,\n",
    "    classes=N_ACTIONS,\n",
    "    classifier_activation='softmax'\n",
    ")\n",
    "\n",
    "target_model = tf.keras.models.clone_model(model)\n",
    "# target_model_weights = model.save_weights(\"./target_model_weights\")\n",
    "TARGET_MODEL_UPDATE_FREQUENCY = 1000\n",
    "tr_weights = model.trainable_weights\n",
    "\n",
    "total_params = sum([np.prod(w.get_shape().as_list()) for w in tr_weights])\n",
    "print(total_params)\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(lr = 2e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_observation(obs):\n",
    "    return obs / OBS_RANGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(env, action): # big but finite MDP\n",
    "    i = 0\n",
    "    k_reward = 0\n",
    "    done = 0\n",
    "    while i < N_INPUT_FRAMES and not done:\n",
    "\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        obs[:, :, i:i+3] = observation # simply concatenate observations --> each is 3 channels deep\n",
    "        i += 1\n",
    "        k_reward += reward\n",
    "\n",
    "    p_obs = preprocess_observation(obs)\n",
    "    return p_obs, k_reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "def process_gradient(g):\n",
    "    return tf.math.maximum(1., tf.math.minimum(-1., g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inside\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-06 21:36:27.645531: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inside\n",
      "inside\n",
      "inside\n",
      "inside\n",
      "inside\n",
      "inside\n",
      "inside\n",
      "inside\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/marcello/github/AtariGym/main.ipynb Cell 7'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/marcello/github/AtariGym/main.ipynb#ch0000005?line=49'>50</a>\u001b[0m \u001b[39m# sample BATCH_SIZE tuples from the buffer\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/marcello/github/AtariGym/main.ipynb#ch0000005?line=50'>51</a>\u001b[0m transition_batch \u001b[39m=\u001b[39m sample(experience_buffer, BATCH_SIZE) \u001b[39m# for the training instead batch is 32\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/marcello/github/AtariGym/main.ipynb#ch0000005?line=52'>53</a>\u001b[0m batched_old_observation \u001b[39m=\u001b[39m     tf\u001b[39m.\u001b[39;49mstack([transition[\u001b[39m0\u001b[39;49m] \u001b[39mfor\u001b[39;49;00m transition \u001b[39min\u001b[39;49;00m transition_batch])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/marcello/github/AtariGym/main.ipynb#ch0000005?line=53'>54</a>\u001b[0m \u001b[39m# batched_action =              tf.stack([transition[1] for transition in transition_batch])\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/marcello/github/AtariGym/main.ipynb#ch0000005?line=54'>55</a>\u001b[0m batched_reward \u001b[39m=\u001b[39m              tf\u001b[39m.\u001b[39mstack([transition[\u001b[39m2\u001b[39m] \u001b[39mfor\u001b[39;00m transition \u001b[39min\u001b[39;00m transition_batch])\n",
      "File \u001b[0;32m~/github/AtariGym/env/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/marcello/github/AtariGym/env/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py?line=147'>148</a>\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/marcello/github/AtariGym/env/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py?line=148'>149</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/marcello/github/AtariGym/env/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py?line=149'>150</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/marcello/github/AtariGym/env/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py?line=150'>151</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    <a href='file:///home/marcello/github/AtariGym/env/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py?line=151'>152</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/github/AtariGym/env/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1082\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/marcello/github/AtariGym/env/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py?line=1079'>1080</a>\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/marcello/github/AtariGym/env/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py?line=1080'>1081</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///home/marcello/github/AtariGym/env/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py?line=1081'>1082</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/marcello/github/AtariGym/env/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py?line=1082'>1083</a>\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m   <a href='file:///home/marcello/github/AtariGym/env/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py?line=1083'>1084</a>\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/marcello/github/AtariGym/env/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py?line=1084'>1085</a>\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/marcello/github/AtariGym/env/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py?line=1085'>1086</a>\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m~/github/AtariGym/env/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:1424\u001b[0m, in \u001b[0;36mstack\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   <a href='file:///home/marcello/github/AtariGym/env/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py?line=1420'>1421</a>\u001b[0m \u001b[39mif\u001b[39;00m axis \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   <a href='file:///home/marcello/github/AtariGym/env/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py?line=1421'>1422</a>\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   <a href='file:///home/marcello/github/AtariGym/env/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py?line=1422'>1423</a>\u001b[0m     \u001b[39m# If the input is a constant list, it can be converted to a constant op\u001b[39;00m\n\u001b[0;32m-> <a href='file:///home/marcello/github/AtariGym/env/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py?line=1423'>1424</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mconvert_to_tensor(values, name\u001b[39m=\u001b[39;49mname)\n\u001b[1;32m   <a href='file:///home/marcello/github/AtariGym/env/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py?line=1424'>1425</a>\u001b[0m   \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m   <a href='file:///home/marcello/github/AtariGym/env/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py?line=1425'>1426</a>\u001b[0m     \u001b[39mpass\u001b[39;00m  \u001b[39m# Input list contains non-constant tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/github/AtariGym/env/lib/python3.8/site-packages/tensorflow/python/profiler/trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/marcello/github/AtariGym/env/lib/python3.8/site-packages/tensorflow/python/profiler/trace.py?line=180'>181</a>\u001b[0m   \u001b[39mwith\u001b[39;00m Trace(trace_name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtrace_kwargs):\n\u001b[1;32m    <a href='file:///home/marcello/github/AtariGym/env/lib/python3.8/site-packages/tensorflow/python/profiler/trace.py?line=181'>182</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> <a href='file:///home/marcello/github/AtariGym/env/lib/python3.8/site-packages/tensorflow/python/profiler/trace.py?line=182'>183</a>\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/github/AtariGym/env/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:1695\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   <a href='file:///home/marcello/github/AtariGym/env/lib/python3.8/site-packages/tensorflow/python/framework/ops.py?line=1689'>1690</a>\u001b[0m       \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mconvert_to_tensor did not convert to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   <a href='file:///home/marcello/github/AtariGym/env/lib/python3.8/site-packages/tensorflow/python/framework/ops.py?line=1690'>1691</a>\u001b[0m                       \u001b[39m\"\u001b[39m\u001b[39mthe preferred dtype: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m vs \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   <a href='file:///home/marcello/github/AtariGym/env/lib/python3.8/site-packages/tensorflow/python/framework/ops.py?line=1691'>1692</a>\u001b[0m                       (ret\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mbase_dtype, preferred_dtype\u001b[39m.\u001b[39mbase_dtype))\n\u001b[1;32m   <a href='file:///home/marcello/github/AtariGym/env/lib/python3.8/site-packages/tensorflow/python/framework/ops.py?line=1693'>1694</a>\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///home/marcello/github/AtariGym/env/lib/python3.8/site-packages/tensorflow/python/framework/ops.py?line=1694'>1695</a>\u001b[0m   ret \u001b[39m=\u001b[39m conversion_func(value, dtype\u001b[39m=\u001b[39;49mdtype, name\u001b[39m=\u001b[39;49mname, as_ref\u001b[39m=\u001b[39;49mas_ref)\n\u001b[1;32m   <a href='file:///home/marcello/github/AtariGym/env/lib/python3.8/site-packages/tensorflow/python/framework/ops.py?line=1696'>1697</a>\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNotImplemented\u001b[39m:\n\u001b[1;32m   <a href='file:///home/marcello/github/AtariGym/env/lib/python3.8/site-packages/tensorflow/python/framework/ops.py?line=1697'>1698</a>\u001b[0m   \u001b[39mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/github/AtariGym/env/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:343\u001b[0m, in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    <a href='file:///home/marcello/github/AtariGym/env/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py?line=339'>340</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_constant_tensor_conversion_function\u001b[39m(v, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    <a href='file:///home/marcello/github/AtariGym/env/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py?line=340'>341</a>\u001b[0m                                          as_ref\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    <a href='file:///home/marcello/github/AtariGym/env/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py?line=341'>342</a>\u001b[0m   _ \u001b[39m=\u001b[39m as_ref\n\u001b[0;32m--> <a href='file:///home/marcello/github/AtariGym/env/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py?line=342'>343</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m constant(v, dtype\u001b[39m=\u001b[39;49mdtype, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[0;32m~/github/AtariGym/env/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:267\u001b[0m, in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    <a href='file:///home/marcello/github/AtariGym/env/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py?line=169'>170</a>\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mconstant\u001b[39m\u001b[39m\"\u001b[39m, v1\u001b[39m=\u001b[39m[])\n\u001b[1;32m    <a href='file:///home/marcello/github/AtariGym/env/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py?line=170'>171</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconstant\u001b[39m(value, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, shape\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConst\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    <a href='file:///home/marcello/github/AtariGym/env/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py?line=171'>172</a>\u001b[0m   \u001b[39m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/marcello/github/AtariGym/env/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py?line=172'>173</a>\u001b[0m \n\u001b[1;32m    <a href='file:///home/marcello/github/AtariGym/env/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py?line=173'>174</a>\u001b[0m \u001b[39m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/marcello/github/AtariGym/env/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py?line=264'>265</a>\u001b[0m \u001b[39m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/marcello/github/AtariGym/env/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py?line=265'>266</a>\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/marcello/github/AtariGym/env/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py?line=266'>267</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m _constant_impl(value, dtype, shape, name, verify_shape\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    <a href='file:///home/marcello/github/AtariGym/env/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py?line=267'>268</a>\u001b[0m                         allow_broadcast\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/github/AtariGym/env/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:279\u001b[0m, in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    <a href='file:///home/marcello/github/AtariGym/env/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py?line=276'>277</a>\u001b[0m     \u001b[39mwith\u001b[39;00m trace\u001b[39m.\u001b[39mTrace(\u001b[39m\"\u001b[39m\u001b[39mtf.constant\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    <a href='file:///home/marcello/github/AtariGym/env/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py?line=277'>278</a>\u001b[0m       \u001b[39mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[0;32m--> <a href='file:///home/marcello/github/AtariGym/env/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py?line=278'>279</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[1;32m    <a href='file:///home/marcello/github/AtariGym/env/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py?line=280'>281</a>\u001b[0m g \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mget_default_graph()\n\u001b[1;32m    <a href='file:///home/marcello/github/AtariGym/env/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py?line=281'>282</a>\u001b[0m tensor_value \u001b[39m=\u001b[39m attr_value_pb2\u001b[39m.\u001b[39mAttrValue()\n",
      "File \u001b[0;32m~/github/AtariGym/env/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:304\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    <a href='file:///home/marcello/github/AtariGym/env/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py?line=301'>302</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_constant_eager_impl\u001b[39m(ctx, value, dtype, shape, verify_shape):\n\u001b[1;32m    <a href='file:///home/marcello/github/AtariGym/env/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py?line=302'>303</a>\u001b[0m   \u001b[39m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/marcello/github/AtariGym/env/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py?line=303'>304</a>\u001b[0m   t \u001b[39m=\u001b[39m convert_to_eager_tensor(value, ctx, dtype)\n\u001b[1;32m    <a href='file:///home/marcello/github/AtariGym/env/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py?line=304'>305</a>\u001b[0m   \u001b[39mif\u001b[39;00m shape \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/marcello/github/AtariGym/env/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py?line=305'>306</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m t\n",
      "File \u001b[0;32m~/github/AtariGym/env/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    <a href='file:///home/marcello/github/AtariGym/env/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py?line=99'>100</a>\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[1;32m    <a href='file:///home/marcello/github/AtariGym/env/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py?line=100'>101</a>\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m--> <a href='file:///home/marcello/github/AtariGym/env/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py?line=101'>102</a>\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_frames = 0\n",
    "target_model_update_frames = 0\n",
    "\n",
    "for _ in range(int(1e6)):                           # n° of episodes\n",
    "\n",
    "      observation, info = env.reset(seed=42, return_info=True)\n",
    "      action = env.action_space.sample() # how do you solve the first frame? which action do you choose? (since the obs are 4 frames concat)\n",
    "\n",
    "      for _ in range(int(1e3)):                     # n° of frames per episode\n",
    "\n",
    "            i = 0\n",
    "            k_reward = 0\n",
    "            done = 0\n",
    "            # sample_k_frames = sample(SKIP_FRAMES, 1)[0]\n",
    "            \n",
    "            while i < N_INPUT_FRAMES and not done:\n",
    "\n",
    "                  observation, reward, done, info = env.step(action)\n",
    "                  obs[:, :, i:i+3] = observation\n",
    "                  i += 1\n",
    "                  k_reward += reward\n",
    "\n",
    "            p_obs = preprocess_observation(obs)\n",
    "\n",
    "            if total_frames < REPLAY_START_SIZE:\n",
    "                  # random sampling the first \"REPLAY_START_SIZE\" steps\n",
    "                  action = env.action_space.sample()\n",
    "                  next_p_obs, reward, is_end_state = simulate(env, action)\n",
    "                  experience_buffer.append((p_obs, action, reward, next_p_obs, is_end_state))\n",
    "\n",
    "            else:\n",
    "                  fraction_frames = (total_frames - REPLAY_START_SIZE) / FINAL_EXPLORATION_FRAME\n",
    "                  eps = max( \n",
    "                        (1-fraction_frames)*INIT_EPS + fraction_frames*END_EPS, \n",
    "                        END_EPS\n",
    "                  )\n",
    "                  # policy\n",
    "                  if np.random.uniform() < eps:\n",
    "                        action = env.action_space.sample()\n",
    "                  else:\n",
    "                        action = max(model([p_obs])) # [p_obs] necessary to give batch=1\n",
    "                        # action = max(model([p_obs for _ in range(BATCH_SIZE)])) # [p_obs] necessary to give batch=1\n",
    "                  \n",
    "                  # next observation\n",
    "                  next_p_obs, reward, is_end_state = simulate(env, action)\n",
    "                  # append observation, action, reward, next_observation and \"done\" into the experience buffer\n",
    "                  experience_buffer.append((p_obs, action, reward, next_p_obs, is_end_state)) # deque automatically pops from opposite side if maxlen is surpassed\n",
    "\n",
    "                  # sample BATCH_SIZE tuples from the buffer\n",
    "                  transition_batch = sample(experience_buffer, BATCH_SIZE) # for the training instead batch is 32\n",
    "\n",
    "                  batched_old_observation =     tf.stack([transition[0] for transition in transition_batch])\n",
    "                  # batched_action =              tf.stack([transition[1] for transition in transition_batch])\n",
    "                  batched_reward =              tf.stack([transition[2] for transition in transition_batch])\n",
    "                  batched_new_observation =     tf.stack([transition[3] for transition in transition_batch])\n",
    "                  batched_done =                tf.stack([transition[4] for transition in transition_batch])\n",
    "\n",
    "                  # compute the max q_values for all x(t+1)\n",
    "                  batch_q_values = tf.math.reduce_max(target_model(batched_new_observation), axis=1)\n",
    "                  # select only the ones in which the model is not in its final state, because the value of the final state's target is simply the final reward\n",
    "\n",
    "                  masked_batch_q_values = tf.multiply(tf.multiply(batch_q_values, tf.cast(batched_done, tf.float32)), DISCOUNT_FACTOR)\n",
    "                  # compute the target for each transition: reward + possibly the max q_value\n",
    "                  target = tf.math.add(masked_batch_q_values, batched_reward)\n",
    "                  # now let's train the network: we need to update the weights\n",
    "\n",
    "                  with tf.GradientTape() as tape:\n",
    "                        # tape.watch(model.tr_w)\n",
    "                        # the loss function is simply MSR\n",
    "                        loss = tf.math.reduce_sum(\n",
    "                              tf.math.square( \n",
    "                                    tf.subtract(\n",
    "                                          target, \n",
    "                                          tf.math.reduce_max( model(batched_old_observation), axis=1 )\n",
    "                                    )  \n",
    "                              )\n",
    "                        )\n",
    "\n",
    "                  grads = tape.gradient(loss, tr_weights)\n",
    "                  # clip the gradients in [-1;+1]\n",
    "                  processed_grads = [process_gradient(g) for g in grads]\n",
    "                  opt.apply_gradients(zip(processed_grads, tr_weights))\n",
    "\n",
    "                  if target_model_update_frames == TARGET_MODEL_UPDATE_FREQUENCY:\n",
    "                        target_model_update_frames = 0\n",
    "                        model.save_weights(\"./weights/update_weights\")\n",
    "                        target_model.load_weights(\"./weights/update_weights\")\n",
    "                        print(\"Total frame\", total_frames, \"updating weights\")\n",
    "                  target_model_update_frames += 1\n",
    "            # if done:\n",
    "            #     observation, info = env.reset(return_info=True)\n",
    "\n",
    "            total_frames += 1\n",
    "      break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
